{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "de0552a3",
   "metadata": {},
   "source": [
    "Importing Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8caada15",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import joblib as jb\n",
    "from sklearn.metrics import recall_score\n",
    "from sklearn.model_selection import GridSearchCV"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "424ef4bd",
   "metadata": {},
   "source": [
    "Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2808a6ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# NOTE : In medical screeing tasks like heart disease prediction, False negatives are far more dangerous than False\n",
    "#        positives. False negative -> model predicts 'No-disease' and patient has disease -> problematic\n",
    "#                   False positive -> model predicts 'disease' when patient is healthy\n",
    "# THEREFORE, it is better to slightly overpredict (high recall, less precise) than to miss severe cases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9b4d3bc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# importing dataset and class weights required for model training ->\n",
    "\n",
    "X_train = pd.read_csv(\"../data/processed/X_train.csv\")\n",
    "X_test = pd.read_csv(\"../data/processed/X_test.csv\")\n",
    "y_train = pd.read_csv(\"../data/processed/y_train.csv\").squeeze() # squeeze method is used to convert df -> 1D array\n",
    "y_test = pd.read_csv(\"../data/processed/y_test.csv\").squeeze()\n",
    "\n",
    "class_weights = { # we will be using weights to minimize loss functions for models that use it\n",
    "    0: 0.4475609756097561,\n",
    "    1: 0.6924528301886792,\n",
    "    3: 1.7069767441860466,\n",
    "    2: 1.7069767441860466,\n",
    "    4: 6.672727272727273\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8de6e835",
   "metadata": {},
   "source": [
    "Logistic Regression Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c603b2ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 10 candidates, totalling 50 fits\n",
      "best parameters :  {'C': 1, 'solver': 'lbfgs'}\n",
      "best score : 42.79%\n",
      "first 10 predictions : [1 3 1 4 0 1 2 4 0 0]\n",
      "recall score : 53.26%\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "lr_model = LogisticRegression(\n",
    "    class_weight=class_weights,\n",
    "    max_iter=1000,# number of optimization iterations\n",
    ")\n",
    "\n",
    "param_grid = {\n",
    "    'solver' : ['lbfgs', 'liblinear'],# solver decides which optimization algorithm to use to find the best model params\n",
    "    'C': [0.01, 0.1, 1, 5, 10]# # C tells about how much error the model can tolerate\n",
    "}\n",
    "\n",
    "grid_search = GridSearchCV(\n",
    "    estimator=lr_model,\n",
    "    param_grid=param_grid,\n",
    "    cv=5,# 5 fold cross-validation\n",
    "    scoring='recall_macro',# gives equal weight to each class -> ideal for multi-class and imbalanced data\n",
    "    # this ensures that minority classes matter as much as majority class (0)\n",
    "    # it is the average of all the recalls of the classes\n",
    "    n_jobs=-1,# use all cores of the processor\n",
    "    verbose=2# print the message while computing\n",
    ")\n",
    "\n",
    "grid_search.fit(X_train,y_train) # fitting of best parameters ->\n",
    "\n",
    "print(\"best parameters : \",grid_search.best_params_)\n",
    "print(f\"best score : {grid_search.best_score_*100:.2f}%\")\n",
    "\n",
    "lr_model = grid_search.best_estimator_\n",
    "y_pred_lr = lr_model.predict(X_test)\n",
    "\n",
    "print(\"first 10 predictions :\",y_pred_lr[:10])\n",
    "print(f\"recall score : {recall_score(y_test,y_pred_lr,average='weighted')*100:.2f}%\")\n",
    "# optional refinement -> we can focus recall on diseased classes only -> if we want to be even more precise,\n",
    "# instead of averaging recall across all 5 classes -> we can calculate recall only for diseased classes(1-4)\n",
    "# this shows exactly how well your model detects patients -> ignoring healthy predictions entirely"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7603057d",
   "metadata": {},
   "source": [
    "KNN classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8e7bd597",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 32 candidates, totalling 160 fits\n",
      "best parameters :  {'metric': 'euclidean', 'n_neighbors': 7, 'weights': 'distance'}\n",
      "best score : 35.71%\n",
      "first 10 predictions :  [1 2 1 3 0 0 0 3 0 1]\n",
      "recall score : 60.33%\n"
     ]
    }
   ],
   "source": [
    "# knn-classifier's performance depends on the values of k and the type of distance we are choosing ->\n",
    "# hence we will do enhancement of the model to select the best option ->\n",
    "\n",
    "# these are the parameters which we want to assign the best values to ->\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "param_grid = {\n",
    "    'metric' : ['euclidean','manhattan'],\n",
    "    'n_neighbors' : [5,7,9,11,13,15,17,19],\n",
    "    'weights' : ['uniform','distance']\n",
    "}\n",
    "\n",
    "grid_search = GridSearchCV(\n",
    "    estimator=KNeighborsClassifier(),\n",
    "    param_grid=param_grid,\n",
    "    cv=5,\n",
    "    scoring='recall_macro', # focus on balanced recall\n",
    "    n_jobs=-1,\n",
    "    verbose=2\n",
    ")\n",
    "\n",
    "grid_search.fit(X_train,y_train)\n",
    "\n",
    "print(\"best parameters : \",grid_search.best_params_)\n",
    "print(f\"best score : {grid_search.best_score_*100:.2f}%\")\n",
    "\n",
    "knn_model = grid_search.best_estimator_\n",
    "y_pred_knn = knn_model.predict(X_test)\n",
    "print(\"first 10 predictions : \",y_pred_knn[:10])\n",
    "print(f\"recall score : {recall_score(y_test,y_pred_knn,average='weighted')*100:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f7fe1b9",
   "metadata": {},
   "source": [
    "Decision Tree Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0de32dfe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 9 candidates, totalling 45 fits\n",
      "best parameters :  {'max_depth': 9}\n",
      "best score : 34.38%\n",
      "[1 3 3 3 0 0 1 2 0 0]\n",
      "recall score : 49.46%\n"
     ]
    }
   ],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "dt_model = DecisionTreeClassifier(\n",
    "    criterion='entropy',# the model will choose Information Gain to decide best feature and threshold for each split\n",
    "    class_weight=class_weights,\n",
    "    random_state=42    \n",
    ")\n",
    "\n",
    "param_grid = {\n",
    "    'max_depth' : [3,4,5,6,7,8,9,11,13]# max-splits the tree can have ->\n",
    "}\n",
    "\n",
    "grid_search = GridSearchCV(\n",
    "    estimator=dt_model,\n",
    "    param_grid=param_grid,\n",
    "    cv=5,\n",
    "    scoring='recall_macro',\n",
    "    n_jobs=-1,\n",
    "    verbose=2\n",
    ")\n",
    "\n",
    "grid_search.fit(X_train,y_train)\n",
    "print(\"best parameters : \",grid_search.best_params_)\n",
    "print(f\"best score : {grid_search.best_score_*100:.2f}%\")\n",
    "\n",
    "dt_model =grid_search.best_estimator_\n",
    "\n",
    "y_pred_dt = dt_model.predict(X_test)\n",
    "print(y_pred_dt[:10])\n",
    "print(f\"recall score : {recall_score(y_test,y_pred_dt,average='weighted')*100:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a923dcd",
   "metadata": {},
   "source": [
    "Random Forest Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "558e6529",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 88 candidates, totalling 440 fits\n",
      "best parameters :  {'max_depth': 3, 'n_estimators': 70}\n",
      "best score : 40.63%\n",
      "first 10 predictions :  [1 3 3 4 0 0 1 4 0 1]\n",
      "recall score : 55.98%\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "rf_model = RandomForestClassifier(\n",
    "    criterion='entropy',\n",
    "    class_weight=class_weights,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "param_grid = {\n",
    "    'n_estimators' : [10,20,30,40,50,60,70,80],# the number of trees in the random forest \n",
    "    'max_depth' : [3,4,5,6,7,8,9,10,11,12,13]# max-splits each tree in the random forest can have\n",
    "}\n",
    "\n",
    "grid_search = GridSearchCV(\n",
    "    estimator=rf_model,\n",
    "    param_grid=param_grid,\n",
    "    cv=5,\n",
    "    scoring='recall_macro',\n",
    "    n_jobs=-1,\n",
    "    verbose=2\n",
    ")\n",
    "\n",
    "grid_search.fit(X_train,y_train)\n",
    "print(\"best parameters : \",grid_search.best_params_)\n",
    "print(f\"best score : {grid_search.best_score_*100:.2f}%\")\n",
    "\n",
    "rf_model = grid_search.best_estimator_\n",
    "\n",
    "y_pred_rf = rf_model.predict(X_test)\n",
    "print(\"first 10 predictions : \",y_pred_rf[:10])\n",
    "print(f\"recall score : {recall_score(y_test,y_pred_rf,average='weighted')*100:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f47fb09",
   "metadata": {},
   "source": [
    "Gradient Boosting Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "20c46679",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 99 candidates, totalling 495 fits\n",
      "best parameters :  {'max_depth': 5, 'n_estimators': 90}\n",
      "best score : 36.85%\n",
      "first 10 predictions :  [1 1 3 3 0 0 0 3 0 1]\n",
      "recall score : 60.33%\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "\n",
    "param_grid = {\n",
    "    'n_estimators' : [10,20,30,40,50,60,70,80,90],\n",
    "    'max_depth' : [3,4,5,6,7,8,9,10,11,12,13]\n",
    "}\n",
    "\n",
    "gb_model = GradientBoostingClassifier(\n",
    "    learning_rate=0.05,\n",
    "    subsample=0.9,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "grid_search = GridSearchCV(\n",
    "    estimator=gb_model,\n",
    "    param_grid=param_grid,\n",
    "    cv=5,\n",
    "    scoring='recall_macro',\n",
    "    n_jobs=-1,\n",
    "    verbose=2\n",
    ")\n",
    "\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "print(\"best parameters : \",grid_search.best_params_)\n",
    "print(f\"best score : {grid_search.best_score_*100:.2f}%\")\n",
    "\n",
    "gb_model = grid_search.best_estimator_\n",
    "\n",
    "y_pred_gb = gb_model.predict(X_test)\n",
    "\n",
    "print(\"first 10 predictions : \",y_pred_gb[:10])\n",
    "print(f\"recall score : {recall_score(y_pred_gb,y_test,average='weighted')*100:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1195b278",
   "metadata": {},
   "source": [
    "Extreme Gradient Boosting Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6686f41a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 440 candidates, totalling 2200 fits\n",
      "best parameters :  {'learning_rate': 0.1, 'max_depth': 4, 'n_estimators': 80}\n",
      "best score : 38.04%\n",
      "first 10 predictions :  [1 2 3 3 0 0 0 3 0 1]\n",
      "recall score : 61.41%\n"
     ]
    }
   ],
   "source": [
    "from xgboost import XGBClassifier\n",
    "\n",
    "xgb_model = XGBClassifier(\n",
    "    subsample=0.9,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "param_grid = {\n",
    "    'n_estimators' : [20,30,40,50,60,70,80,90],\n",
    "    'max_depth' : [3,4,5,6,7,8,9,10,11,12,13],\n",
    "    'learning_rate' : [0.01,0.05,0.1,0.15,0.2]\n",
    "}\n",
    "\n",
    "grid_search = GridSearchCV(\n",
    "   estimator=xgb_model,\n",
    "   param_grid=param_grid,\n",
    "   cv=5,\n",
    "   scoring='recall_macro',\n",
    "   n_jobs=-1,\n",
    "   verbose=2 \n",
    ")\n",
    "\n",
    "grid_search.fit(X_train,y_train)\n",
    "\n",
    "print(\"best parameters : \",grid_search.best_params_)\n",
    "print(f\"best score : {grid_search.best_score_*100:.2f}%\")\n",
    "\n",
    "xgb_model = grid_search.best_estimator_\n",
    "\n",
    "y_pred_xgb = xgb_model.predict(X_test)\n",
    "\n",
    "print(\"first 10 predictions : \",y_pred_xgb[:10])\n",
    "print(f\"recall score : {recall_score(y_test,y_pred_xgb,average='weighted')*100:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24b60bf6",
   "metadata": {},
   "source": [
    "Naive Bayes Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5f0346f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "first 10 predictions :  [1 2 1 3 0 1 0 3 0 1]\n",
      "recall score : 55.98%\n"
     ]
    }
   ],
   "source": [
    "from sklearn.naive_bayes import GaussianNB\n",
    "\n",
    "nb_model = GaussianNB()\n",
    "\n",
    "nb_model.fit(X_train,y_train)\n",
    "\n",
    "y_pred_nb = nb_model.predict(X_test)\n",
    "\n",
    "print(\"first 10 predictions : \",y_pred_nb[:10])\n",
    "print(f\"recall score : {recall_score(y_test,y_pred_nb,average='weighted')*100:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1212d22f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import joblib as jb\n",
    "# saving the models using joblib for integration and deployment ->\n",
    "\n",
    "jb.dump(lr_model,\"../models/lr_model.pkl\")\n",
    "jb.dump(knn_model,\"../models/knn_model.pkl\")\n",
    "jb.dump(dt_model,\"../models/dt_model.pkl\")\n",
    "jb.dump(rf_model,\"../models/rf_model.pkl\")\n",
    "jb.dump(gb_model,\"../models/gb_model.pkl\")\n",
    "jb.dump(xgb_model,\"../models/xgb_model.pkl\")\n",
    "jb.dump(nb_model,\"../models/nb_model.pkl\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "my_env_312",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
