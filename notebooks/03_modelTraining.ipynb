{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "de0552a3",
   "metadata": {},
   "source": [
    "Importing Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8caada15",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import joblib as jb\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "424ef4bd",
   "metadata": {},
   "source": [
    "Model Training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab114dd5",
   "metadata": {},
   "source": [
    "IMPORTANT NOTE :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2808a6ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. while model training, I have tuned the hyperparameters of each model such that the train-test accuracies \n",
    "#    are not only high but also approximately equal.\n",
    "# 2. This ensured that the models did not overfit the training data while still maintaining good predictive \n",
    "#    performance on unseen samples. \n",
    "# => Therefore the goal/focus is to maximize generalization rather than memorization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9b4d3bc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# importing dataset and class weights required for model training ->\n",
    "\n",
    "X_train = pd.read_csv(\"../data/processed/X_train.csv\")\n",
    "X_test = pd.read_csv(\"../data/processed/X_test.csv\")\n",
    "y_train = pd.read_csv(\"../data/processed/y_train.csv\").squeeze() # squeeze method is used to convert df -> 1D array\n",
    "y_test = pd.read_csv(\"../data/processed/y_test.csv\").squeeze()\n",
    "\n",
    "class_weights = {\n",
    "    0: 0.4475609756097561,\n",
    "    1: 0.6924528301886792,\n",
    "    3: 1.7069767441860466,\n",
    "    2: 1.7069767441860466,\n",
    "    4: 6.672727272727273\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8de6e835",
   "metadata": {},
   "source": [
    "Logistic Regression Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c603b2ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "first 10 predictions : [1 3 1 4 0 0 2 4 0 0]\n",
      "train accuracy : 57.36%\n",
      "test accuracy : 53.80%\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "lr_model = LogisticRegression(\n",
    "    class_weight=class_weights,\n",
    "    max_iter=1000,\n",
    "    solver='lbfgs',\n",
    ")\n",
    "\n",
    "lr_model.fit(X_train,y_train)\n",
    "\n",
    "y_pred_lr = lr_model.predict(X_test)\n",
    "\n",
    "print(\"first 10 predictions :\",y_pred_lr[:10])\n",
    "\n",
    "print(f\"train accuracy : {lr_model.score(X_train,y_train)*100:.2f}%\")\n",
    "accuracy_lr = accuracy_score(y_test,y_pred_lr)\n",
    "print(f\"test accuracy : {accuracy_lr*100:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7603057d",
   "metadata": {},
   "source": [
    "KNN classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8e7bd597",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "best k value : 19\n",
      "best distance metric : manhattan\n",
      "train accuracy : 58.99%\n",
      "test accuracy : 59.24%\n"
     ]
    }
   ],
   "source": [
    "# knn-classifier's performance depends on the values of k and the type of distance we are choosing ->\n",
    "# hence we will do enhancement of the model to select the best option ->\n",
    "\n",
    "# these are the parameters which we want to assign the best values to ->\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "best_k = None\n",
    "best_distance = None\n",
    "best_accuracy_knn = 0\n",
    "best_knn_model = None\n",
    "\n",
    "# example execution of Grid Search for calculation of best hyperparameters ->\n",
    "for dist in ['manhattan','euclidean']: # possible distance metrics \n",
    "    for k in [3,5,7,11,13,15,17,19,21,23,25,27]: # possible k values\n",
    "        knn_model = KNeighborsClassifier(\n",
    "            n_neighbors=k,\n",
    "            weights='uniform',\n",
    "            metric=dist\n",
    "        )\n",
    "        knn_model.fit(X_train,y_train)\n",
    "        y_pred_knn = knn_model.predict(X_test)\n",
    "        accuracy_knn = accuracy_score(y_test,y_pred_knn)\n",
    "        if(accuracy_knn>best_accuracy_knn):\n",
    "            best_accuracy_knn=accuracy_knn\n",
    "            best_k = k\n",
    "            best_distance = dist\n",
    "            best_knn_model = knn_model\n",
    "\n",
    "print(f\"best k value : {best_k}\")\n",
    "print(f\"best distance metric : {best_distance}\")\n",
    "print(f\"train accuracy : {best_knn_model.score(X_train,y_train)*100:.2f}%\")\n",
    "print(f\"test accuracy : {best_accuracy_knn*100:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "583ed061",
   "metadata": {},
   "source": [
    "Analysis of Entropy of the Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "736aaad3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "max entropy possible : 2.3219\n",
      "entropy of the dataset : 1.9133\n",
      "purity of the dataset : 17.60%\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "# Entropy is the measure of impurity of a dataset \n",
    "# Entropy of the dataset -> Summation{pi*log2(pi)}\n",
    "y_train.value_counts() # we find the number of samples to each class\n",
    "counts = [328,212,86,86,22]\n",
    "total = sum(counts)\n",
    "\n",
    "# computing probabilities ->\n",
    "probs = [count/total for count in counts]\n",
    "\n",
    "# compute entropy ->\n",
    "entropy = -sum(p * math.log2(p) for p in probs if p>0)\n",
    "\n",
    "print(f\"max entropy possible : {math.log2(5):.4f}\")\n",
    "print(f\"entropy of the dataset : {entropy:.4f}\")\n",
    "print(f\"purity of the dataset : {(1-(entropy/math.log2(5)))*100:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f7fe1b9",
   "metadata": {},
   "source": [
    "Decision Tree Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "0de32dfe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "first 10 predictions : [1 1 0 4 0 0 1 1 0 1]\n",
      "train accuracy : 55.45%\n",
      "test accuracy : 55.98%\n"
     ]
    }
   ],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "dt_model = DecisionTreeClassifier(\n",
    "    criterion='entropy',# the model with choose Information Gain to decide best feature and threshold for each split\n",
    "    class_weight=class_weights,\n",
    "    max_depth=3,# the model will choose the best 3 attributes in terms of IG for growing the tree\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "# note :  as stated earlier, I have calculated max_depth hyperparameter of the decisionTreeClassifier by manual search \n",
    "#         to minimize difference in train-test accuracy and at the same time keep them high for generalization\n",
    "\n",
    "dt_model.fit(X_train,y_train)\n",
    "\n",
    "y_pred_dt = dt_model.predict(X_test)\n",
    "\n",
    "print(\"first 10 predictions :\",y_pred_dt[:10])\n",
    "print(f\"train accuracy : {dt_model.score(X_train,y_train)*100:.2f}%\",)\n",
    "accuracy_dt = accuracy_score(y_test,y_pred_dt)\n",
    "print(f\"test accuracy : {accuracy_dt*100:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a923dcd",
   "metadata": {},
   "source": [
    "Random Forest Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "558e6529",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "first 10 predictions : [1 3 3 4 0 0 0 4 0 1]\n",
      "train accuracy : 59.95%\n",
      "test accuracy : 59.24%\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "rf_model = RandomForestClassifier(\n",
    "    criterion='entropy',\n",
    "    n_estimators=11,# \n",
    "    max_depth=3,\n",
    "    class_weight=class_weights,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "rf_model.fit(X_train,y_train)\n",
    "\n",
    "y_pred_rf = rf_model.predict(X_test)\n",
    "\n",
    "print(\"first 10 predictions :\",y_pred_rf[:10])\n",
    "print(f\"train accuracy : {rf_model.score(X_train,y_train)*100:.2f}%\",)\n",
    "accuracy_rf = accuracy_score(y_test,y_pred_rf)\n",
    "print(f\"test accuracy : {accuracy_rf*100:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f47fb09",
   "metadata": {},
   "source": [
    "Gradient Boosting Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "20c46679",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Accuracy: 62.67%\n",
      "Test Accuracy: 63.59%\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "\n",
    "gb_model = GradientBoostingClassifier(\n",
    "    n_estimators=14,\n",
    "    learning_rate=0.05,\n",
    "    max_depth=3,\n",
    "    min_samples_split=2,\n",
    "    min_samples_leaf=1,\n",
    "    subsample=0.9,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "gb_model.fit(X_train, y_train)\n",
    "\n",
    "y_pred_gb = gb_model.predict(X_test)\n",
    "\n",
    "train_acc = gb_model.score(X_train, y_train)\n",
    "test_acc = accuracy_score(y_test, y_pred_gb)\n",
    "\n",
    "print(f\"Train Accuracy: {train_acc*100:.2f}%\")\n",
    "print(f\"Test Accuracy: {test_acc*100:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1076c23e",
   "metadata": {},
   "source": [
    "Support Vector Machines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "1feb7362",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Accuracy: 59.81%\n",
      "Test Accuracy: 59.78%\n"
     ]
    }
   ],
   "source": [
    "from sklearn.svm import SVC\n",
    "svm_model = SVC(\n",
    "    kernel='rbf',         # RBF kernel (non-linear)\n",
    "    C=1.0,                # Regularization strength\n",
    "    gamma='scale',        # Auto gamma for RBF\n",
    "    decision_function_shape='ovo',  # one-vs-one (for multi-class)\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "# Train\n",
    "svm_model.fit(X_train, y_train)\n",
    "\n",
    "# Predict\n",
    "y_pred_svm = svm_model.predict(X_test)\n",
    "\n",
    "# Evaluate\n",
    "train_acc = svm_model.score(X_train, y_train)\n",
    "test_acc = accuracy_score(y_test, y_pred_svm)\n",
    "\n",
    "print(f\"Train Accuracy: {train_acc*100:.2f}%\")\n",
    "print(f\"Test Accuracy: {test_acc*100:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1212d22f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import joblib as jb\n",
    "# saving the models using joblib for integration and deployment ->\n",
    "\n",
    "jb.dump(lr_model,\"../models/lr_model.pkl\")\n",
    "jb.dump(best_knn_model,\"../models/knn_model.pkl\")\n",
    "jb.dump(dt_model,\"../models/dt_model.pkl\")\n",
    "jb.dump(rf_model,\"../models/rf_model.pkl\")\n",
    "jb.dump(gb_model,\"../models/gb_model.pkl\")\n",
    "jb.dump(svm_model,\"../models/svm_model.pkl\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "my_env_312",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
