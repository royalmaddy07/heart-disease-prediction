{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "81e13c84",
   "metadata": {},
   "source": [
    "Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "16c55d72",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import sklearn"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e522d0e",
   "metadata": {},
   "source": [
    "Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "dce5a251",
   "metadata": {},
   "outputs": [],
   "source": [
    "# data collection ->\n",
    "\n",
    "df = pd.read_csv(\"../data/raw/heart_disease_encoded.csv\")\n",
    "\n",
    "# 0. removing un-necessary features -> \n",
    "\n",
    "df = df.drop(columns=['id']) # id does not carry any meaningful information for model training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7156d93",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. HANDLING MISSING VALUES(NaN) ->\n",
    "\n",
    "df.isna().sum() # o/p -> number of missing values for each column\n",
    "# result : the following columns have missing values -> \n",
    "# trestbps, chol, fbs, restecg, thalch, exang, oldpeak, slope, ca, thal\n",
    "\n",
    "missing_features = ['trestbps','chol','fbs','restecg','thalch','exang','oldpeak','slope','ca','thal']\n",
    "categorical_features = ['sex', 'cp', 'fbs', 'restecg', 'exang', 'slope', 'ca', 'thal', 'target']\n",
    "continuous_features = ['age', 'trestbps', 'chol', 'thalch', 'oldpeak']\n",
    "\n",
    "\n",
    "# CONTEXT -> in the images directory you can check in the univariate analysis folder that non of the\n",
    "#            features follow normal distribution. \n",
    "# INFERENCE -> median -> measure of central tendency for continuous features\n",
    "#              mode -> measure of central tendency for categorical features\n",
    "\n",
    "# function to handle missing values ->\n",
    "def fill_missing_values(df,categorical_f,continuous_f):\n",
    "    for feature in df.columns:\n",
    "        if(feature in categorical_f):\n",
    "            df[feature] = df[feature].fillna(value=df[feature].mode()[0])\n",
    "        elif(feature in continuous_f) :\n",
    "            df[feature] = df[feature].fillna(value=df[feature].median())\n",
    "\n",
    "fill_missing_values(df,categorical_features,continuous_features)\n",
    "\n",
    "df.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d8c81ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. DROPING ANY DUPLICATE ROWS IN THE df ->\n",
    "print(df.shape)\n",
    "# originally -> there were 920 rows\n",
    "df = df.drop_duplicates()\n",
    "\n",
    "df.reset_index(drop=True,inplace=True)# dropping duplicates doesnot reset duplicates -> hence we have to reset index\n",
    "print(df.shape)\n",
    "# current -> there are 918 rows -> 2 duplicate rows are dropped"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2dd0fe5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. OUTLIER HANDLING -> METHOD USED -> IQR method and caping outlier values with boundary values ->\n",
    "#                                    -> prevents values from biasing model while keeping the data intact\n",
    "\n",
    "# concept -> only continuous features can have outlier values -> \n",
    "print(continuous_features)\n",
    "display(df.describe())\n",
    "\n",
    "for feature in continuous_features:\n",
    "    Q1 = df[feature].quantile(0.25)\n",
    "    Q3 = df[feature].quantile(0.75)\n",
    "    IQR = Q3-Q1\n",
    "    UB = Q3 + 1.5*IQR\n",
    "    LB = Q1 - 1.5*IQR# calculation of upper and lower bounds for each continuous feature\n",
    "\n",
    "    capped_upper = (df[feature] > UB).sum()# calculating number of values greater than upper bound\n",
    "    capped_lower = (df[feature] < LB).sum()# calculating number of values smaller than lower bound\n",
    "    df[feature] = np.where(df[feature]>UB, UB, np.where(df[feature]<LB, LB, df[feature]))# outlier handling\n",
    "\n",
    "    print(f\"{feature} IQR range between {LB:.2f} and {UB:.2f}\")\n",
    "    print(f\"{feature}: capped {capped_upper} upper and {capped_lower} lower values\")\n",
    "    print(\"-\"*30)\n",
    "\n",
    "display(df.describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11a725d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. TRAIN-TEST SPLITING OF DATA ->\n",
    "\n",
    "# checking class imbalance for target label -> \n",
    "df['num(target)'].value_counts(normalize=True)*100\n",
    "# there is a class imbalance in the label -> there is a considerable amount of difference in the sample size \n",
    "# b/w class 0 and class 4 -> we need to take that into consideration \n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "display(df.shape)\n",
    "X = df.drop(columns=['num(target)'],axis=1) # X contains the features for model training\n",
    "display(X.shape)\n",
    "y = df['num(target)'] # Y contains the target label\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(# train-test split method ->\n",
    "    X,y,\n",
    "    test_size=0.2,\n",
    "    random_state=42,\n",
    "    stratify=y # stratify y since there is an imbalance in class for target label\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a86bc96",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5. MIN-MAX FEATURE SCALING -> scaling continuous features to values from 0 to 1\n",
    "# concept -> never apply min-max scaling before spliting -> since scaling requires min and max values -> this creates\n",
    "# data leakage and will give unrealistically high accuracy\n",
    "\n",
    "display(X_train, X_test) # these are the dataframes to scale\n",
    "print(continuous_features) # these are the features in the dataframe which we want to scale\n",
    "\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "scaler = MinMaxScaler(feature_range=(0,1))\n",
    "\n",
    "scaler.fit(X_train[continuous_features])# learning min and max values from training data only\n",
    "\n",
    "X_train[continuous_features] = scaler.transform(X_train[continuous_features])\n",
    "X_test[continuous_features] = scaler.transform(X_test[continuous_features])\n",
    "\n",
    "display(X_train)\n",
    "display(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74ba41b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6. HANDLING CLASS IMBALANCE -> using class weights\n",
    "display(y_train.value_counts())\n",
    "# our training dataset is imbalanced -> class 4 is a minority class\n",
    "\n",
    "class_counts = y_train.value_counts()\n",
    "total_count = len(y_train)\n",
    "k = y_train.nunique()\n",
    "\n",
    "class_weights = {}\n",
    "\n",
    "display(class_counts,total_count,k)\n",
    "\n",
    "for cls, count in class_counts.items():\n",
    "    wt = total_count / (k * count)\n",
    "    class_weights[cls] = wt\n",
    "\n",
    "class_weights"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "my_env_312",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
